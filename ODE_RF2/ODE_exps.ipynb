{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ODE_RF2.MCV_ode import *\n",
    "from ODE_RF2.ODE_Solver import *\n",
    "from ODE_RF2.ODE_tasks import *\n",
    "from ODE_RF2.NCV_ode import *\n",
    "from src.score_funcs import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/2000. loss: 27.964061641693114\n",
      "20/2000. loss: 21.188690662384033\n",
      "30/2000. loss: 12.677747011184692\n",
      "40/2000. loss: 12.535665941238403\n",
      "50/2000. loss: 12.744404196739197\n",
      "60/2000. loss: 21.768822813034056\n",
      "70/2000. loss: 15.956471920013428\n",
      "80/2000. loss: 10.481371402740479\n",
      "90/2000. loss: 13.0963019490242\n",
      "100/2000. loss: 13.423962235450745\n",
      "110/2000. loss: 18.772275340557098\n",
      "120/2000. loss: 6.970543944835663\n",
      "130/2000. loss: 21.925162899494172\n",
      "140/2000. loss: 26.01908077597618\n",
      "150/2000. loss: 21.545803451538085\n",
      "160/2000. loss: 11.288484168052673\n",
      "170/2000. loss: 17.702935111522674\n",
      "180/2000. loss: 9.004867553710938\n",
      "190/2000. loss: 19.095006251335143\n",
      "200/2000. loss: 11.365452361106872\n",
      "210/2000. loss: 9.856925737857818\n",
      "220/2000. loss: 12.022565698623657\n",
      "230/2000. loss: 9.009490966796875\n",
      "240/2000. loss: 8.31558816432953\n",
      "250/2000. loss: 6.329714465141296\n",
      "260/2000. loss: 5.860865497589112\n",
      "270/2000. loss: 14.462975597381591\n",
      "280/2000. loss: 8.722365808486938\n",
      "290/2000. loss: 8.367925333976746\n",
      "300/2000. loss: 13.995799994468689\n",
      "310/2000. loss: 8.756836795806885\n",
      "320/2000. loss: 9.746864700317383\n",
      "330/2000. loss: 11.861076712608337\n",
      "340/2000. loss: 8.455054020881652\n",
      "350/2000. loss: 6.841532230377197\n",
      "360/2000. loss: 9.372007703781128\n",
      "370/2000. loss: 5.877351093292236\n",
      "380/2000. loss: 5.373084104061126\n",
      "390/2000. loss: 6.250904154777527\n",
      "400/2000. loss: 8.68918068408966\n",
      "410/2000. loss: 8.966675806045533\n",
      "420/2000. loss: 7.908208322525025\n",
      "430/2000. loss: 5.785313081741333\n",
      "440/2000. loss: 6.310750246047974\n",
      "450/2000. loss: 5.877783083915711\n",
      "460/2000. loss: 7.615179014205933\n",
      "470/2000. loss: 6.322964501380921\n",
      "480/2000. loss: 8.178517818450928\n",
      "490/2000. loss: 7.686507320404052\n",
      "500/2000. loss: 5.740432333946228\n",
      "510/2000. loss: 6.8774611234664915\n",
      "520/2000. loss: 6.5350980997085575\n",
      "530/2000. loss: 5.750617480278015\n",
      "540/2000. loss: 5.318844485282898\n",
      "550/2000. loss: 4.902945017814636\n",
      "560/2000. loss: 5.235591244697571\n",
      "570/2000. loss: 4.861558508872986\n",
      "580/2000. loss: 5.296263408660889\n",
      "590/2000. loss: 5.916217422485351\n",
      "600/2000. loss: 6.336172437667846\n",
      "610/2000. loss: 4.432199382781983\n",
      "620/2000. loss: 5.1032480001449585\n",
      "630/2000. loss: 5.718083381652832\n",
      "640/2000. loss: 4.899272012710571\n",
      "650/2000. loss: 6.160555839538574\n",
      "660/2000. loss: 4.652864074707031\n",
      "670/2000. loss: 4.574366021156311\n",
      "680/2000. loss: 4.81257848739624\n",
      "690/2000. loss: 5.318037271499634\n",
      "700/2000. loss: 4.369633936882019\n",
      "710/2000. loss: 4.417646741867065\n",
      "720/2000. loss: 4.919160938262939\n",
      "730/2000. loss: 5.400881767272949\n",
      "740/2000. loss: 4.972554016113281\n",
      "750/2000. loss: 5.470224213600159\n",
      "760/2000. loss: 3.713795578479767\n",
      "770/2000. loss: 5.137755560874939\n",
      "780/2000. loss: 4.716885495185852\n",
      "790/2000. loss: 4.416311478614807\n",
      "800/2000. loss: 4.316062712669373\n",
      "810/2000. loss: 4.896160697937011\n",
      "820/2000. loss: 5.835268354415893\n",
      "830/2000. loss: 5.59267510175705\n",
      "840/2000. loss: 4.174347639083862\n",
      "850/2000. loss: 5.2687856435775755\n",
      "860/2000. loss: 3.6419692039489746\n",
      "870/2000. loss: 3.2263230204582216\n",
      "880/2000. loss: 4.052649593353271\n",
      "890/2000. loss: 4.289733076095581\n",
      "900/2000. loss: 4.280968499183655\n",
      "910/2000. loss: 3.7384934186935426\n",
      "920/2000. loss: 3.4250797510147093\n",
      "930/2000. loss: 4.82711091041565\n",
      "940/2000. loss: 3.9360000252723695\n",
      "950/2000. loss: 8.262507057189941\n",
      "960/2000. loss: 4.441109824180603\n",
      "970/2000. loss: 3.506403112411499\n",
      "980/2000. loss: 2.921907067298889\n",
      "990/2000. loss: 4.482856893539429\n",
      "1000/2000. loss: 8.05183253288269\n",
      "1010/2000. loss: 4.796035718917847\n",
      "1020/2000. loss: 4.972117805480957\n",
      "1030/2000. loss: 5.3820641040802\n",
      "1040/2000. loss: 3.152381980419159\n",
      "1050/2000. loss: 3.6119264364242554\n",
      "1060/2000. loss: 3.029399645328522\n",
      "1070/2000. loss: 3.462691378593445\n",
      "1080/2000. loss: 2.850847804546356\n",
      "1090/2000. loss: 5.028088438510895\n",
      "1100/2000. loss: 3.5894613027572633\n",
      "1110/2000. loss: 4.988479995727539\n",
      "1120/2000. loss: 3.894536757469177\n",
      "1130/2000. loss: 5.754311215877533\n",
      "1140/2000. loss: 3.3218441724777223\n",
      "1150/2000. loss: 3.190465009212494\n",
      "1160/2000. loss: 3.151117467880249\n",
      "1170/2000. loss: 3.6147801637649537\n",
      "1180/2000. loss: 2.4775787472724913\n",
      "1190/2000. loss: 4.203091204166412\n",
      "1200/2000. loss: 2.5108161509037017\n",
      "1210/2000. loss: 4.159536695480346\n",
      "1220/2000. loss: 3.103410315513611\n",
      "1230/2000. loss: 3.37987105846405\n",
      "1240/2000. loss: 3.095209503173828\n",
      "1250/2000. loss: 3.1930259227752686\n",
      "1260/2000. loss: 2.722669780254364\n",
      "1270/2000. loss: 2.9954254388809205\n",
      "1280/2000. loss: 2.977411377429962\n",
      "1290/2000. loss: 3.5236146569252016\n",
      "1300/2000. loss: 2.585351037979126\n",
      "1310/2000. loss: 4.607352948188781\n",
      "1320/2000. loss: 3.388794779777527\n",
      "1330/2000. loss: 2.9011730909347535\n",
      "1340/2000. loss: 2.465825688838959\n",
      "1350/2000. loss: 2.6341450691223143\n",
      "1360/2000. loss: 3.740724968910217\n",
      "1370/2000. loss: 2.7605592727661135\n",
      "1380/2000. loss: 2.305934488773346\n",
      "1390/2000. loss: 2.615594208240509\n",
      "1400/2000. loss: 2.630621302127838\n",
      "1410/2000. loss: 3.7845737814903258\n",
      "1420/2000. loss: 2.6293014168739317\n",
      "1430/2000. loss: 2.533380091190338\n",
      "1440/2000. loss: 1.9325710654258728\n",
      "1450/2000. loss: 2.8086232423782347\n",
      "1460/2000. loss: 2.4436997890472414\n",
      "1470/2000. loss: 1.870296263694763\n",
      "1480/2000. loss: 2.164634883403778\n",
      "1490/2000. loss: 2.266700232028961\n",
      "1500/2000. loss: 2.399733817577362\n",
      "1510/2000. loss: 2.1205235362052917\n",
      "1520/2000. loss: 1.6650538980960845\n",
      "1530/2000. loss: 2.5870530009269714\n",
      "1540/2000. loss: 3.010126107931137\n",
      "1550/2000. loss: 3.1168476700782777\n",
      "1560/2000. loss: 2.5836653470993043\n",
      "1570/2000. loss: 2.5721877455711364\n",
      "1580/2000. loss: 1.7159115433692933\n",
      "1590/2000. loss: 2.0612677931785583\n",
      "1600/2000. loss: 2.0384400844573975\n",
      "1610/2000. loss: 2.517267656326294\n",
      "1620/2000. loss: 1.25979026556015\n",
      "1630/2000. loss: 1.7371085107326507\n",
      "1640/2000. loss: 2.1843250572681425\n",
      "1650/2000. loss: 2.067774760723114\n",
      "1660/2000. loss: 2.164043605327606\n",
      "1670/2000. loss: 2.9367910861968993\n",
      "1680/2000. loss: 1.6029856264591218\n",
      "1690/2000. loss: 2.006493103504181\n",
      "1700/2000. loss: 1.8839837551116942\n",
      "1710/2000. loss: 1.5630837321281432\n",
      "1720/2000. loss: 1.629107904434204\n",
      "1730/2000. loss: 2.0499486088752747\n",
      "1740/2000. loss: 2.6298382759094237\n",
      "1750/2000. loss: 10.957213139533996\n",
      "1760/2000. loss: 1.6523917078971864\n",
      "1770/2000. loss: 1.55126234292984\n",
      "1780/2000. loss: 1.6119250297546386\n",
      "1790/2000. loss: 1.5843101799488069\n",
      "1800/2000. loss: 1.6367890238761902\n",
      "1810/2000. loss: 1.6385049939155578\n",
      "1820/2000. loss: 1.8474319279193878\n",
      "1830/2000. loss: 1.5047495782375335\n",
      "1840/2000. loss: 1.7702597439289094\n",
      "1850/2000. loss: 2.4411082923412324\n",
      "1860/2000. loss: 1.442707711458206\n",
      "1870/2000. loss: 2.8070704698562623\n",
      "1880/2000. loss: 4.657582247257233\n",
      "1890/2000. loss: 6.098792982101441\n",
      "1900/2000. loss: 4.170201921463013\n",
      "1910/2000. loss: 2.3929747104644776\n",
      "1920/2000. loss: 1.3910614132881165\n",
      "1930/2000. loss: 2.7541893422603607\n",
      "1940/2000. loss: 1.5844427287578582\n",
      "1950/2000. loss: 1.5050127387046814\n",
      "1960/2000. loss: 1.4077240228652954\n",
      "1970/2000. loss: 1.154728850722313\n",
      "1980/2000. loss: 1.8963516294956206\n",
      "1990/2000. loss: 1.0485634684562684\n",
      "2000/2000. loss: 2.9603210508823397\n",
      "Meta_testing finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.6281172, 1.639386572688818, 2.1095383)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case\n",
    "K = 2\n",
    "score_dict  = {'SCORE':multivariate_Normal_score,'mean':torch.zeros(1,1), 'cov':torch.ones(1,1)}\n",
    "tasks = ODE_Task_Distribution(a_min=0, a_max=1)\n",
    "# MCV\n",
    "inlr = 0.01\n",
    "instep=1\n",
    "mCV2 = MetaNeuralCV_ode('ode', MetaNeuralCVModel_ode, D_in=1, h_dims=[80] * 3, init_val=torch.tensor([0.]), weight_decay=5.e-6,\\\n",
    "                   tasks=tasks, inner_optim=torch.optim.Adam, inner_lr=inlr, meta_lr=2e-3, K=K, inner_steps=instep,\\\n",
    "                   tasks_per_meta_batch=5,\\\n",
    "                   **score_dict)\n",
    "mCV2.main_loop(num_iterations=2000)\n",
    "mCV2.test(100, inner_optim=torch.optim.Adam, inner_steps=instep)\n",
    "np.mean(np.array(mCV2.log_test[0]['Abserr_CVests'])), np.mean(np.array(mCV2.log_test[0]['Abserr_MC_2m_ests'])), np.mean(np.array(mCV2.log_test[0]['Abserr_MCests']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100\n",
      "11/100\n",
      "21/100\n",
      "31/100\n",
      "41/100\n",
      "51/100\n",
      "61/100\n",
      "71/100\n",
      "81/100\n",
      "91/100\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([1.9039, 2.5945])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NCV\n",
    "K = 2\n",
    "score_dict  = {'SCORE':multivariate_Normal_score,'mean':torch.zeros(1,1), 'cov':torch.ones(1,1)}\n",
    "tasks = ODE_Task_Distribution(a_min=0, a_max=1)\n",
    "out2 = torch.zeros(100,2)\n",
    "for i in range(100):\n",
    "    if i%10 ==0:\n",
    "        print('{}/{}'.format(i+1,100))\n",
    "    nCV = NeuralCV_ode('ode', NeuralCVModel_ode, D_in=1, h_dims=[80] * 3, init_val=torch.tensor([0.]), weight_decay=5.e-6,\\\n",
    "                   tasks=tasks, optim=torch.optim.Adam, lr=2e-3, K=K,\\\n",
    "                   **score_dict)\n",
    "    __ = nCV.train_val(num_epochs=20, batch_size=5, verbose=False)\n",
    "    out2[i,0] = torch.tensor(__[0])\n",
    "    out2[i,1] = torch.tensor(__[1])\n",
    "\n",
    "# Mean Absolute error: MC_2m abs err | NCV_abs_err\n",
    "out2.mean(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/2000. loss: 6.405482888221741\n",
      "20/2000. loss: 9.47003014087677\n",
      "30/2000. loss: 5.125806927680969\n",
      "40/2000. loss: 4.045700979232788\n",
      "50/2000. loss: 6.918171024322509\n",
      "60/2000. loss: 5.7333495140075685\n",
      "70/2000. loss: 5.931632328033447\n",
      "80/2000. loss: 7.117025089263916\n",
      "90/2000. loss: 7.409681487083435\n",
      "100/2000. loss: 5.002206206321716\n",
      "110/2000. loss: 3.072193944454193\n",
      "120/2000. loss: 4.15521365404129\n",
      "130/2000. loss: 3.8294225215911863\n",
      "140/2000. loss: 2.6726217389106752\n",
      "150/2000. loss: 3.3400380849838256\n",
      "160/2000. loss: 3.219298708438873\n",
      "170/2000. loss: 3.155245637893677\n",
      "180/2000. loss: 3.035214531421661\n",
      "190/2000. loss: 2.9733695864677427\n",
      "200/2000. loss: 3.6352470397949217\n",
      "210/2000. loss: 2.7944488525390625\n",
      "220/2000. loss: 2.4435361504554747\n",
      "230/2000. loss: 3.386460375785828\n",
      "240/2000. loss: 3.2463273882865904\n",
      "250/2000. loss: 2.0984551191329954\n",
      "260/2000. loss: 2.0766526818275453\n",
      "270/2000. loss: 2.1459009408950807\n",
      "280/2000. loss: 2.4143054604530336\n",
      "290/2000. loss: 2.620494747161865\n",
      "300/2000. loss: 2.3217971205711363\n",
      "310/2000. loss: 2.8328383207321166\n",
      "320/2000. loss: 2.1073296427726746\n",
      "330/2000. loss: 2.288240885734558\n",
      "340/2000. loss: 2.0028370141983034\n",
      "350/2000. loss: 2.0790951013565064\n",
      "360/2000. loss: 2.229633867740631\n",
      "370/2000. loss: 2.028828227519989\n",
      "380/2000. loss: 2.6703482031822205\n",
      "390/2000. loss: 1.9576270580291748\n",
      "400/2000. loss: 1.866592037677765\n",
      "410/2000. loss: 1.6133154273033141\n",
      "420/2000. loss: 2.649045741558075\n",
      "430/2000. loss: 1.657476544380188\n",
      "440/2000. loss: 1.8422484993934631\n",
      "450/2000. loss: 1.7758716464042663\n",
      "460/2000. loss: 1.837973129749298\n",
      "470/2000. loss: 1.7011221647262573\n",
      "480/2000. loss: 1.5409165501594544\n",
      "490/2000. loss: 1.7027733087539674\n",
      "500/2000. loss: 1.6768890142440795\n",
      "510/2000. loss: 1.8251095056533813\n",
      "520/2000. loss: 1.9796800374984742\n",
      "530/2000. loss: 1.8381529211997987\n",
      "540/2000. loss: 1.682477730512619\n",
      "550/2000. loss: 1.5654687762260437\n",
      "560/2000. loss: 1.5117062211036683\n",
      "570/2000. loss: 1.5459129691123963\n",
      "580/2000. loss: 1.3371512293815613\n",
      "590/2000. loss: 1.6674342632293702\n",
      "600/2000. loss: 1.596711814403534\n",
      "610/2000. loss: 1.8344167172908783\n",
      "620/2000. loss: 1.396594226360321\n",
      "630/2000. loss: 1.4993669629096984\n",
      "640/2000. loss: 1.1471363604068756\n",
      "650/2000. loss: 1.608291757106781\n",
      "660/2000. loss: 1.296932828426361\n",
      "670/2000. loss: 1.7915697395801544\n",
      "680/2000. loss: 4.102729654312133\n",
      "690/2000. loss: 5.889288878440857\n",
      "700/2000. loss: 6.5067966222763065\n",
      "710/2000. loss: 5.89881044626236\n",
      "720/2000. loss: 3.3225889563560487\n",
      "730/2000. loss: 5.391336250305176\n",
      "740/2000. loss: 4.872349798679352\n",
      "750/2000. loss: 4.868837380409241\n",
      "760/2000. loss: 2.7303104400634766\n",
      "770/2000. loss: 2.76395765542984\n",
      "780/2000. loss: 2.683662784099579\n",
      "790/2000. loss: 2.2088647723197936\n",
      "800/2000. loss: 2.5719722867012025\n",
      "810/2000. loss: 1.6267988979816437\n",
      "820/2000. loss: 1.8948396861553192\n",
      "830/2000. loss: 1.796775233745575\n",
      "840/2000. loss: 1.8496024847030639\n",
      "850/2000. loss: 1.5007945597171783\n",
      "860/2000. loss: 1.2943370938301086\n",
      "870/2000. loss: 1.723208451271057\n",
      "880/2000. loss: 1.6782784342765809\n",
      "890/2000. loss: 1.4545804917812348\n",
      "900/2000. loss: 1.5381098866462708\n",
      "910/2000. loss: 1.143631899356842\n",
      "920/2000. loss: 1.2042782366275788\n",
      "930/2000. loss: 1.1787577390670776\n",
      "940/2000. loss: 1.3936814755201339\n",
      "950/2000. loss: 1.6854693651199342\n",
      "960/2000. loss: 1.6149113655090332\n",
      "970/2000. loss: 1.5071574449539185\n",
      "980/2000. loss: 1.6746704220771789\n",
      "990/2000. loss: 1.9377796053886414\n",
      "1000/2000. loss: 1.755355679988861\n",
      "1010/2000. loss: 1.2789248883724214\n",
      "1020/2000. loss: 1.3102798461914062\n",
      "1030/2000. loss: 1.0359470009803773\n",
      "1040/2000. loss: 1.211328959465027\n",
      "1050/2000. loss: 1.4212812602519989\n",
      "1060/2000. loss: 1.0344973862171174\n",
      "1070/2000. loss: 0.9417978882789612\n",
      "1080/2000. loss: 0.955895847082138\n",
      "1090/2000. loss: 1.0822897553443909\n",
      "1100/2000. loss: 1.1146572947502136\n",
      "1110/2000. loss: 1.3129785716533662\n",
      "1120/2000. loss: 1.0888997912406921\n",
      "1130/2000. loss: 1.148344373703003\n",
      "1140/2000. loss: 1.0177972197532654\n",
      "1150/2000. loss: 1.0246779561042785\n",
      "1160/2000. loss: 1.0308814287185668\n",
      "1170/2000. loss: 1.0251285672187804\n",
      "1180/2000. loss: 1.011477029323578\n",
      "1190/2000. loss: 1.2236846148967744\n",
      "1200/2000. loss: 0.9494825005531311\n",
      "1210/2000. loss: 1.0333349585533143\n",
      "1220/2000. loss: 1.2160123467445374\n",
      "1230/2000. loss: 0.902532023191452\n",
      "1240/2000. loss: 0.9148150384426117\n",
      "1250/2000. loss: 1.326540195941925\n",
      "1260/2000. loss: 1.237962532043457\n",
      "1270/2000. loss: 0.8141029953956604\n",
      "1280/2000. loss: 1.1831161439418794\n",
      "1290/2000. loss: 0.9828733026981353\n",
      "1300/2000. loss: 0.864124196767807\n",
      "1310/2000. loss: 1.0327607691287994\n",
      "1320/2000. loss: 0.8547869563102722\n",
      "1330/2000. loss: 0.8473252594470978\n",
      "1340/2000. loss: 0.8757345080375671\n",
      "1350/2000. loss: 0.7617162257432938\n",
      "1360/2000. loss: 1.240198701620102\n",
      "1370/2000. loss: 1.368977278470993\n",
      "1380/2000. loss: 0.8878431737422943\n",
      "1390/2000. loss: 1.069302785396576\n",
      "1400/2000. loss: 1.4714114785194397\n",
      "1410/2000. loss: 1.7485289871692657\n",
      "1420/2000. loss: 1.878037440776825\n",
      "1430/2000. loss: 1.6137872219085694\n",
      "1440/2000. loss: 1.4574729204177856\n",
      "1450/2000. loss: 1.4944831728935242\n",
      "1460/2000. loss: 1.362111783027649\n",
      "1470/2000. loss: 1.3136584162712097\n",
      "1480/2000. loss: 1.3680359959602355\n",
      "1490/2000. loss: 1.0122189462184905\n",
      "1500/2000. loss: 1.0390592873096467\n",
      "1510/2000. loss: 1.0831094980239868\n",
      "1520/2000. loss: 1.0165569186210632\n",
      "1530/2000. loss: 0.8342341244220733\n",
      "1540/2000. loss: 0.9349276602268219\n",
      "1550/2000. loss: 0.9916073203086853\n",
      "1560/2000. loss: 0.8184051960706711\n",
      "1570/2000. loss: 0.896639609336853\n",
      "1580/2000. loss: 0.9141072571277619\n",
      "1590/2000. loss: 0.9208932757377625\n",
      "1600/2000. loss: 0.8856341183185578\n",
      "1610/2000. loss: 0.8605355203151703\n",
      "1620/2000. loss: 1.1439147472381592\n",
      "1630/2000. loss: 1.0575130105018615\n",
      "1640/2000. loss: 0.8667366981506348\n",
      "1650/2000. loss: 1.0543712317943572\n",
      "1660/2000. loss: 1.0242026627063752\n",
      "1670/2000. loss: 0.7823619544506073\n",
      "1680/2000. loss: 0.7044287085533142\n",
      "1690/2000. loss: 0.6950311362743378\n",
      "1700/2000. loss: 0.8643824815750122\n",
      "1710/2000. loss: 0.8522896587848663\n",
      "1720/2000. loss: 0.8784289836883545\n",
      "1730/2000. loss: 0.8107789993286133\n",
      "1740/2000. loss: 1.0087408483028413\n",
      "1750/2000. loss: 0.7867302566766738\n",
      "1760/2000. loss: 0.8850313127040863\n",
      "1770/2000. loss: 0.7831483900547027\n",
      "1780/2000. loss: 1.1334270715713501\n",
      "1790/2000. loss: 0.7782111525535583\n",
      "1800/2000. loss: 0.8160275280475616\n",
      "1810/2000. loss: 1.113241583108902\n",
      "1820/2000. loss: 1.1110338509082793\n",
      "1830/2000. loss: 1.3277019321918488\n",
      "1840/2000. loss: 1.0419786870479584\n",
      "1850/2000. loss: 0.9722124993801117\n",
      "1860/2000. loss: 0.8581888616085053\n",
      "1870/2000. loss: 0.9392550528049469\n",
      "1880/2000. loss: 0.6512553304433822\n",
      "1890/2000. loss: 0.6397242993116379\n",
      "1900/2000. loss: 0.7445130944252014\n",
      "1910/2000. loss: 0.6839295327663422\n",
      "1920/2000. loss: 0.6866830796003341\n",
      "1930/2000. loss: 0.6870323300361634\n",
      "1940/2000. loss: 0.6761553943157196\n",
      "1950/2000. loss: 0.6850886642932892\n",
      "1960/2000. loss: 0.8381659090518951\n",
      "1970/2000. loss: 0.6852936118841171\n",
      "1980/2000. loss: 0.7014939427375794\n",
      "1990/2000. loss: 0.986677598953247\n",
      "2000/2000. loss: 0.5599516183137894\n",
      "Meta_testing finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.3266676, 1.1745471835136414, 1.8635556)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case\n",
    "K = 5\n",
    "score_dict  = {'SCORE':multivariate_Normal_score,'mean':torch.zeros(1,1), 'cov':torch.ones(1,1)}\n",
    "tasks = ODE_Task_Distribution(a_min=0, a_max=1) # is good when Oscillatory_Task_Distribution(2, 4.5, 5.5, 0.4, 0.6, 0, 1)\n",
    "# MCV\n",
    "inlr = 0.01\n",
    "instep=1\n",
    "mCV5 = MetaNeuralCV_ode('ode', MetaNeuralCVModel_ode, D_in=1, h_dims=[80] * 3, init_val=torch.tensor([0.]), weight_decay=5.e-6,\\\n",
    "                   tasks=tasks, inner_optim=torch.optim.Adam, inner_lr=inlr, meta_lr=2e-3, K=K, inner_steps=instep,\\\n",
    "                   tasks_per_meta_batch=5,\\\n",
    "                   **score_dict)\n",
    "mCV5.main_loop(num_iterations=2000)\n",
    "mCV5.test(100, inner_optim=torch.optim.Adam, inner_steps=instep)\n",
    "np.mean(np.array(mCV5.log_test[0]['Abserr_CVests'])), np.mean(np.array(mCV5.log_test[0]['Abserr_MC_2m_ests'])), np.mean(np.array(mCV5.log_test[0]['Abserr_MCests']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100\n",
      "11/100\n",
      "21/100\n",
      "31/100\n",
      "41/100\n",
      "51/100\n",
      "61/100\n",
      "71/100\n",
      "81/100\n",
      "91/100\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([1.2412, 1.8477])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NCV\n",
    "K = 5\n",
    "score_dict  = {'SCORE':multivariate_Normal_score,'mean':torch.zeros(1,1), 'cov':torch.ones(1,1)}\n",
    "tasks = ODE_Task_Distribution(a_min=0, a_max=1) # is good when Oscillatory_Task_Distribution(2, 4.5, 5.5, 0.4, 0.6, 0, 1)\n",
    "out5 = torch.zeros(100,2)\n",
    "for i in range(100):\n",
    "    if i%10 ==0:\n",
    "        print('{}/{}'.format(i+1,100))\n",
    "    nCV = NeuralCV_ode('ode', NeuralCVModel_ode, D_in=1, h_dims=[80] * 3, init_val=torch.tensor([0.]), weight_decay=5.e-6, \\\n",
    "                   tasks=tasks, optim=torch.optim.Adam, lr=2e-3, K=K,\\\n",
    "                   **score_dict)\n",
    "    __ = nCV.train_val(num_epochs=20, batch_size=5, verbose=False)\n",
    "    out5[i,0] = torch.tensor(__[0])\n",
    "    out5[i,1] = torch.tensor(__[1])\n",
    "\n",
    "# Mean Absolute error: MC_2m abs err | NCV_abs_err\n",
    "out5.mean(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/2000. loss: 3.177042245864868\n",
      "20/2000. loss: 3.082046723365784\n",
      "30/2000. loss: 3.339434015750885\n",
      "40/2000. loss: 2.7455761671066283\n",
      "50/2000. loss: 3.6430867671966554\n",
      "60/2000. loss: 3.334888231754303\n",
      "70/2000. loss: 3.0025827407836916\n",
      "80/2000. loss: 2.427233576774597\n",
      "90/2000. loss: 1.4846380472183227\n",
      "100/2000. loss: 1.9288481175899506\n",
      "110/2000. loss: 1.6043316841125488\n",
      "120/2000. loss: 1.852189201116562\n",
      "130/2000. loss: 1.2289263606071472\n",
      "140/2000. loss: 1.398149985074997\n",
      "150/2000. loss: 1.4612697243690491\n",
      "160/2000. loss: 1.318725085258484\n",
      "170/2000. loss: 1.3912202894687653\n",
      "180/2000. loss: 1.38212993144989\n",
      "190/2000. loss: 1.4941279947757722\n",
      "200/2000. loss: 1.221729612350464\n",
      "210/2000. loss: 1.1172419786453247\n",
      "220/2000. loss: 1.1511993765830995\n",
      "230/2000. loss: 1.2665098786354065\n",
      "240/2000. loss: 1.0146846234798432\n",
      "250/2000. loss: 0.970200926065445\n",
      "260/2000. loss: 1.117679613828659\n",
      "270/2000. loss: 1.2543274223804475\n",
      "280/2000. loss: 0.9357702732086182\n",
      "290/2000. loss: 1.279207068681717\n",
      "300/2000. loss: 1.0055996477603912\n",
      "310/2000. loss: 0.9895443856716156\n",
      "320/2000. loss: 0.8482177019119262\n",
      "330/2000. loss: 0.9205706477165222\n",
      "340/2000. loss: 0.9288170576095581\n",
      "350/2000. loss: 0.8304845035076142\n",
      "360/2000. loss: 0.9464920103549957\n",
      "370/2000. loss: 0.8403877794742585\n",
      "380/2000. loss: 0.9561866998672486\n",
      "390/2000. loss: 0.8696969330310822\n",
      "400/2000. loss: 0.8080037415027619\n",
      "410/2000. loss: 0.7808810651302338\n",
      "420/2000. loss: 0.7649281978607178\n",
      "430/2000. loss: 0.7600793451070785\n",
      "440/2000. loss: 0.7023733198642731\n",
      "450/2000. loss: 0.8976868748664856\n",
      "460/2000. loss: 0.7348825693130493\n",
      "470/2000. loss: 0.7841288179159165\n",
      "480/2000. loss: 0.7409494757652283\n",
      "490/2000. loss: 0.8169454157352447\n",
      "500/2000. loss: 0.6798058390617371\n",
      "510/2000. loss: 0.7103931427001953\n",
      "520/2000. loss: 0.7028458774089813\n",
      "530/2000. loss: 0.781744310259819\n",
      "540/2000. loss: 0.6773590207099914\n",
      "550/2000. loss: 0.7341133683919907\n",
      "560/2000. loss: 0.7343969404697418\n",
      "570/2000. loss: 0.6568539321422577\n",
      "580/2000. loss: 0.6585377216339111\n",
      "590/2000. loss: 0.6859784871339798\n",
      "600/2000. loss: 0.6639389872550965\n",
      "610/2000. loss: 0.5566473990678787\n",
      "620/2000. loss: 0.5481708496809006\n",
      "630/2000. loss: 0.6299299091100693\n",
      "640/2000. loss: 0.6453516781330109\n",
      "650/2000. loss: 0.5393162339925766\n",
      "660/2000. loss: 0.670387914776802\n",
      "670/2000. loss: 0.5819473385810852\n",
      "680/2000. loss: 0.5972265273332595\n",
      "690/2000. loss: 0.5827259838581085\n",
      "700/2000. loss: 0.6313726544380188\n",
      "710/2000. loss: 0.5233281344175339\n",
      "720/2000. loss: 0.6420370727777481\n",
      "730/2000. loss: 0.5932876199483872\n",
      "740/2000. loss: 0.5662599265575409\n",
      "750/2000. loss: 0.454396715760231\n",
      "760/2000. loss: 0.5128364562988281\n",
      "770/2000. loss: 0.5365891754627228\n",
      "780/2000. loss: 0.4539110064506531\n",
      "790/2000. loss: 0.5121061712503433\n",
      "800/2000. loss: 0.4721326231956482\n",
      "810/2000. loss: 0.49805001020431516\n",
      "820/2000. loss: 0.4750329226255417\n",
      "830/2000. loss: 0.5402622938156127\n",
      "840/2000. loss: 0.41922495514154434\n",
      "850/2000. loss: 0.34150597751140593\n",
      "860/2000. loss: 0.4939347684383392\n",
      "870/2000. loss: 0.41416444480419157\n",
      "880/2000. loss: 0.46993131935596466\n",
      "890/2000. loss: 0.4750582754611969\n",
      "900/2000. loss: 0.5090761691331863\n",
      "910/2000. loss: 0.3961218222975731\n",
      "920/2000. loss: 0.45979651510715486\n",
      "930/2000. loss: 0.46936911046504975\n",
      "940/2000. loss: 0.43448383212089536\n",
      "950/2000. loss: 0.4007098346948624\n",
      "960/2000. loss: 0.3910649359226227\n",
      "970/2000. loss: 0.3809814751148224\n",
      "980/2000. loss: 0.39695610105991364\n",
      "990/2000. loss: 0.3717761874198914\n",
      "1000/2000. loss: 0.35277927219867705\n",
      "1010/2000. loss: 0.36585554778575896\n",
      "1020/2000. loss: 0.38287983536720277\n",
      "1030/2000. loss: 0.3428715839982033\n",
      "1040/2000. loss: 0.5398972362279892\n",
      "1050/2000. loss: 0.3442408353090286\n",
      "1060/2000. loss: 0.4589961767196655\n",
      "1070/2000. loss: 0.5444958716630935\n",
      "1080/2000. loss: 0.624678498506546\n",
      "1090/2000. loss: 0.4130508124828339\n",
      "1100/2000. loss: 0.7357701122760772\n",
      "1110/2000. loss: 0.42246992886066437\n",
      "1120/2000. loss: 0.32359917759895324\n",
      "1130/2000. loss: 0.39491563141345976\n",
      "1140/2000. loss: 0.4012363463640213\n",
      "1150/2000. loss: 0.26917346864938735\n",
      "1160/2000. loss: 0.40458953380584717\n",
      "1170/2000. loss: 0.2779733896255493\n",
      "1180/2000. loss: 0.2815992921590805\n",
      "1190/2000. loss: 0.26967514455318453\n",
      "1200/2000. loss: 0.34951377511024473\n",
      "1210/2000. loss: 0.4631151378154755\n",
      "1220/2000. loss: 0.2683845028281212\n",
      "1230/2000. loss: 0.2877001851797104\n",
      "1240/2000. loss: 0.31081054657697677\n",
      "1250/2000. loss: 0.27039435505867004\n",
      "1260/2000. loss: 0.29389619529247285\n",
      "1270/2000. loss: 0.4381168708205223\n",
      "1280/2000. loss: 0.4147416323423386\n",
      "1290/2000. loss: 0.2593124732375145\n",
      "1300/2000. loss: 0.4658374935388565\n",
      "1310/2000. loss: 0.3170047879219055\n",
      "1320/2000. loss: 0.26840732544660567\n",
      "1330/2000. loss: 0.3186322495341301\n",
      "1340/2000. loss: 0.408536759018898\n",
      "1350/2000. loss: 0.6158571243286133\n",
      "1360/2000. loss: 0.33815314471721647\n",
      "1370/2000. loss: 0.3205980286002159\n",
      "1380/2000. loss: 0.36391410380601885\n",
      "1390/2000. loss: 0.22708892375230788\n",
      "1400/2000. loss: 0.17002310305833818\n",
      "1410/2000. loss: 0.26570568084716795\n",
      "1420/2000. loss: 0.2190597414970398\n",
      "1430/2000. loss: 0.21639586314558984\n",
      "1440/2000. loss: 0.23191586881875992\n",
      "1450/2000. loss: 0.34272836744785307\n",
      "1460/2000. loss: 0.16794362738728524\n",
      "1470/2000. loss: 0.20300979018211365\n",
      "1480/2000. loss: 0.11969871707260608\n",
      "1490/2000. loss: 0.25521885231137276\n",
      "1500/2000. loss: 0.25784415304660796\n",
      "1510/2000. loss: 0.20990164279937745\n",
      "1520/2000. loss: 0.1974077545106411\n",
      "1530/2000. loss: 0.18843387365341185\n",
      "1540/2000. loss: 0.17750653401017188\n",
      "1550/2000. loss: 0.25057184770703317\n",
      "1560/2000. loss: 0.16461486741900444\n",
      "1570/2000. loss: 0.19438210278749465\n",
      "1580/2000. loss: 0.14647666215896607\n",
      "1590/2000. loss: 0.13961149379611015\n",
      "1600/2000. loss: 0.1835765078663826\n",
      "1610/2000. loss: 0.16542737632989885\n",
      "1620/2000. loss: 0.19327298924326897\n",
      "1630/2000. loss: 0.14856996312737464\n",
      "1640/2000. loss: 0.19756505489349366\n",
      "1650/2000. loss: 0.21628771722316742\n",
      "1660/2000. loss: 0.14219240397214888\n",
      "1670/2000. loss: 0.20698398873209953\n",
      "1680/2000. loss: 0.13758239075541495\n",
      "1690/2000. loss: 0.16459380388259887\n",
      "1700/2000. loss: 0.11717092134058475\n",
      "1710/2000. loss: 0.17820906788110732\n",
      "1720/2000. loss: 0.13238271549344063\n",
      "1730/2000. loss: 0.1755795069038868\n",
      "1740/2000. loss: 0.09199375919997692\n",
      "1750/2000. loss: 0.1660702645778656\n",
      "1760/2000. loss: 0.12764386981725692\n",
      "1770/2000. loss: 0.12075367271900177\n",
      "1780/2000. loss: 0.09398868307471275\n",
      "1790/2000. loss: 0.09427073560655116\n",
      "1800/2000. loss: 0.21062950789928436\n",
      "1810/2000. loss: 0.13446422070264816\n",
      "1820/2000. loss: 0.13000147864222528\n",
      "1830/2000. loss: 0.1004454717040062\n",
      "1840/2000. loss: 0.12263234630227089\n",
      "1850/2000. loss: 0.18372175991535186\n",
      "1860/2000. loss: 0.15589419715106487\n",
      "1870/2000. loss: 0.0887721586972475\n",
      "1880/2000. loss: 0.15018708296120167\n",
      "1890/2000. loss: 0.13652637004852294\n",
      "1900/2000. loss: 0.14188029170036315\n",
      "1910/2000. loss: 0.13633214235305785\n",
      "1920/2000. loss: 0.07887645624577999\n",
      "1930/2000. loss: 0.10636411383748054\n",
      "1940/2000. loss: 0.08172487691044808\n",
      "1950/2000. loss: 0.09306640326976776\n",
      "1960/2000. loss: 0.09793467782437801\n",
      "1970/2000. loss: 0.0674870528280735\n",
      "1980/2000. loss: 0.0789429321885109\n",
      "1990/2000. loss: 0.08601634949445724\n",
      "2000/2000. loss: 0.07804538607597351\n",
      "Meta_testing finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.13130632, 0.7187614250183105, 1.0869296)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case\n",
    "K = 10\n",
    "score_dict  = {'SCORE':multivariate_Normal_score,'mean':torch.zeros(1,1), 'cov':torch.ones(1,1)}\n",
    "tasks = ODE_Task_Distribution(a_min=0, a_max=1) # is good when Oscillatory_Task_Distribution(2, 4.5, 5.5, 0.4, 0.6, 0, 1)\n",
    "# MCV\n",
    "inlr = 0.01\n",
    "instep=1\n",
    "mCV10 = MetaNeuralCV_ode('ode', MetaNeuralCVModel_ode, D_in=1, h_dims=[80] * 3, init_val=torch.tensor([0.]), weight_decay=5.e-6,\\\n",
    "                   tasks=tasks, inner_optim=torch.optim.Adam, inner_lr=inlr, meta_lr=2e-3, K=K, inner_steps=instep,\\\n",
    "                   tasks_per_meta_batch=5,\\\n",
    "                   **score_dict)\n",
    "mCV10.main_loop(num_iterations=2000)\n",
    "mCV10.test(100, inner_optim=torch.optim.Adam, inner_steps=instep)\n",
    "np.mean(np.array(mCV10.log_test[0]['Abserr_CVests'])), np.mean(np.array(mCV10.log_test[0]['Abserr_MC_2m_ests'])), np.mean(np.array(mCV10.log_test[0]['Abserr_MCests']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100\n",
      "11/100\n",
      "21/100\n",
      "31/100\n",
      "41/100\n",
      "51/100\n",
      "61/100\n",
      "71/100\n",
      "81/100\n",
      "91/100\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([0.6819, 1.0566])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NCV\n",
    "K = 10\n",
    "score_dict  = {'SCORE':multivariate_Normal_score,'mean':torch.zeros(1,1), 'cov':torch.ones(1,1)}\n",
    "tasks = ODE_Task_Distribution(a_min=0, a_max=1)\n",
    "out10 = torch.zeros(100,2)\n",
    "for i in range(100):\n",
    "    if i%10 ==0:\n",
    "        print('{}/{}'.format(i+1,100))\n",
    "    nCV = NeuralCV_ode('ode', NeuralCVModel_ode, D_in=1, h_dims=[80] * 3, init_val=torch.tensor([0.]), weight_decay=5.e-6,\\\n",
    "                   tasks=tasks, optim=torch.optim.Adam, lr=2e-3, K=K,\\\n",
    "                   **score_dict)\n",
    "    __ = nCV.train_val(num_epochs=20, batch_size=5, verbose=False)\n",
    "    out10[i,0] = torch.tensor(__[0])\n",
    "    out10[i,1] = torch.tensor(__[1])\n",
    "\n",
    "# Mean Absolute error: MC_2m abs err | NCV_abs_err\n",
    "out10.mean(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/2000. loss: 2.0309908032417296\n",
      "20/2000. loss: 1.3955675542354584\n",
      "30/2000. loss: 1.9864452600479126\n",
      "40/2000. loss: 1.570352518558502\n",
      "50/2000. loss: 1.4640998780727386\n",
      "60/2000. loss: 1.1865670263767243\n",
      "70/2000. loss: 1.0090335667133332\n",
      "80/2000. loss: 0.8371401906013489\n",
      "90/2000. loss: 0.6370036184787751\n",
      "100/2000. loss: 0.7862550139427185\n",
      "110/2000. loss: 0.8866901874542237\n",
      "120/2000. loss: 0.7345927774906158\n",
      "130/2000. loss: 0.6958224236965179\n",
      "140/2000. loss: 0.598203843832016\n",
      "150/2000. loss: 0.6022523581981659\n",
      "160/2000. loss: 0.584757125377655\n",
      "170/2000. loss: 0.5543266862630845\n",
      "180/2000. loss: 0.7098299145698548\n",
      "190/2000. loss: 0.6004450678825378\n",
      "200/2000. loss: 0.6592648148536682\n",
      "210/2000. loss: 0.8441999137401581\n",
      "220/2000. loss: 0.6182867616415024\n",
      "230/2000. loss: 0.7203179210424423\n",
      "240/2000. loss: 0.4911184966564178\n",
      "250/2000. loss: 0.5411743849515915\n",
      "260/2000. loss: 0.47035386264324186\n",
      "270/2000. loss: 0.511503267288208\n",
      "280/2000. loss: 0.5395145028829574\n",
      "290/2000. loss: 0.47135973870754244\n",
      "300/2000. loss: 0.5181752741336823\n",
      "310/2000. loss: 0.5169278413057328\n",
      "320/2000. loss: 0.5723868101835251\n",
      "330/2000. loss: 0.4764390647411346\n",
      "340/2000. loss: 0.4659398317337036\n",
      "350/2000. loss: 0.48209924399852755\n",
      "360/2000. loss: 0.4776736319065094\n",
      "370/2000. loss: 0.45363820195198057\n",
      "380/2000. loss: 0.4596230834722519\n",
      "390/2000. loss: 0.4100333571434021\n",
      "400/2000. loss: 0.3730261266231537\n",
      "410/2000. loss: 0.3780667930841446\n",
      "420/2000. loss: 0.3988050282001495\n",
      "430/2000. loss: 0.3944932609796524\n",
      "440/2000. loss: 0.3486673176288605\n",
      "450/2000. loss: 0.41382949352264403\n",
      "460/2000. loss: 0.4813667356967926\n",
      "470/2000. loss: 0.41997659802436826\n",
      "480/2000. loss: 0.3874572187662125\n",
      "490/2000. loss: 0.3463848739862442\n",
      "500/2000. loss: 0.4120695650577545\n",
      "510/2000. loss: 0.3532302528619766\n",
      "520/2000. loss: 0.4400770753622055\n",
      "530/2000. loss: 0.31908227801322936\n",
      "540/2000. loss: 0.3770299434661865\n",
      "550/2000. loss: 0.38926597982645034\n",
      "560/2000. loss: 0.3244002729654312\n",
      "570/2000. loss: 0.3556217819452286\n",
      "580/2000. loss: 0.3533665955066681\n",
      "590/2000. loss: 0.3089429274201393\n",
      "600/2000. loss: 0.36471024304628374\n",
      "610/2000. loss: 0.30884693264961244\n",
      "620/2000. loss: 0.3016495585441589\n",
      "630/2000. loss: 0.29253608286380767\n",
      "640/2000. loss: 0.32429406344890593\n",
      "650/2000. loss: 0.32065656781196594\n",
      "660/2000. loss: 0.28440719842910767\n",
      "670/2000. loss: 0.27373604029417037\n",
      "680/2000. loss: 0.2821596547961235\n",
      "690/2000. loss: 0.2612561255693436\n",
      "700/2000. loss: 0.2625016629695892\n",
      "710/2000. loss: 0.2664609730243683\n",
      "720/2000. loss: 0.2564232885837555\n",
      "730/2000. loss: 0.2760788857936859\n",
      "740/2000. loss: 0.2793802171945572\n",
      "750/2000. loss: 0.25947741121053697\n",
      "760/2000. loss: 0.2740798190236092\n",
      "770/2000. loss: 0.21433961838483812\n",
      "780/2000. loss: 0.22996158599853517\n",
      "790/2000. loss: 0.2574883386492729\n",
      "800/2000. loss: 0.24145097732543946\n",
      "810/2000. loss: 0.22800939679145812\n",
      "820/2000. loss: 0.2156489834189415\n",
      "830/2000. loss: 0.237776455283165\n",
      "840/2000. loss: 0.2076163813471794\n",
      "850/2000. loss: 0.22017810940742494\n",
      "860/2000. loss: 0.2018275648355484\n",
      "870/2000. loss: 0.217471444606781\n",
      "880/2000. loss: 0.23259473443031312\n",
      "890/2000. loss: 0.20812104493379593\n",
      "900/2000. loss: 0.23073350191116332\n",
      "910/2000. loss: 0.2396281212568283\n",
      "920/2000. loss: 0.17511526942253114\n",
      "930/2000. loss: 0.24599051475524902\n",
      "940/2000. loss: 0.20330558717250824\n",
      "950/2000. loss: 0.18569269627332688\n",
      "960/2000. loss: 0.20264571011066437\n",
      "970/2000. loss: 0.19817371815443038\n",
      "980/2000. loss: 0.20827343612909316\n",
      "990/2000. loss: 0.1575662337243557\n",
      "1000/2000. loss: 0.19116702526807786\n",
      "1010/2000. loss: 0.1490102156996727\n",
      "1020/2000. loss: 0.1631605312228203\n",
      "1030/2000. loss: 0.17709186151623726\n",
      "1040/2000. loss: 0.18653720170259475\n",
      "1050/2000. loss: 0.1677320793271065\n",
      "1060/2000. loss: 0.1582239769399166\n",
      "1070/2000. loss: 0.17418685108423232\n",
      "1080/2000. loss: 0.16781341582536696\n",
      "1090/2000. loss: 0.16499270498752594\n",
      "1100/2000. loss: 0.1785900190472603\n",
      "1110/2000. loss: 0.1502685949206352\n",
      "1120/2000. loss: 0.14015005826950072\n",
      "1130/2000. loss: 0.15807782858610153\n",
      "1140/2000. loss: 0.15219778269529344\n",
      "1150/2000. loss: 0.14270122945308686\n",
      "1160/2000. loss: 0.15321378782391548\n",
      "1170/2000. loss: 0.13851140886545182\n",
      "1180/2000. loss: 0.14891123473644258\n",
      "1190/2000. loss: 0.13479409143328666\n",
      "1200/2000. loss: 0.12110684663057328\n",
      "1210/2000. loss: 0.13802375718951226\n",
      "1220/2000. loss: 0.14102769270539284\n",
      "1230/2000. loss: 0.1301318220794201\n",
      "1240/2000. loss: 0.12829143479466437\n",
      "1250/2000. loss: 0.1249934583902359\n",
      "1260/2000. loss: 0.0983689397573471\n",
      "1270/2000. loss: 0.1228395164012909\n",
      "1280/2000. loss: 0.1160904623568058\n",
      "1290/2000. loss: 0.12330916225910186\n",
      "1300/2000. loss: 0.12661598101258278\n",
      "1310/2000. loss: 0.12109033018350601\n",
      "1320/2000. loss: 0.1314152806997299\n",
      "1330/2000. loss: 0.11620603203773498\n",
      "1340/2000. loss: 0.09373974725604058\n",
      "1350/2000. loss: 0.12336459457874298\n",
      "1360/2000. loss: 0.10684657990932464\n",
      "1370/2000. loss: 0.1107112042605877\n",
      "1380/2000. loss: 0.09660345837473869\n",
      "1390/2000. loss: 0.10160258673131466\n",
      "1400/2000. loss: 0.09688878208398818\n",
      "1410/2000. loss: 0.10229741260409356\n",
      "1420/2000. loss: 0.09799926280975342\n",
      "1430/2000. loss: 0.10766005143523216\n",
      "1440/2000. loss: 0.09526266381144524\n",
      "1450/2000. loss: 0.08402002230286598\n",
      "1460/2000. loss: 0.08849643878638744\n",
      "1470/2000. loss: 0.07943499907851219\n",
      "1480/2000. loss: 0.09200355261564255\n",
      "1490/2000. loss: 0.09755900204181671\n",
      "1500/2000. loss: 0.10060529485344887\n",
      "1510/2000. loss: 0.08973995558917522\n",
      "1520/2000. loss: 0.08817138113081455\n",
      "1530/2000. loss: 0.09797796458005906\n",
      "1540/2000. loss: 0.10016695149242878\n",
      "1550/2000. loss: 0.07936809957027435\n",
      "1560/2000. loss: 0.06675281785428525\n",
      "1570/2000. loss: 0.09520617350935937\n",
      "1580/2000. loss: 0.08677901029586792\n",
      "1590/2000. loss: 0.07257701158523559\n",
      "1600/2000. loss: 0.08527301959693431\n",
      "1610/2000. loss: 0.055308041721582414\n",
      "1620/2000. loss: 0.07054380513727665\n",
      "1630/2000. loss: 0.07718891836702824\n",
      "1640/2000. loss: 0.07925971560180187\n",
      "1650/2000. loss: 0.07102088592946529\n",
      "1660/2000. loss: 0.08629185855388641\n",
      "1670/2000. loss: 0.06870837360620499\n",
      "1680/2000. loss: 0.05884634628891945\n",
      "1690/2000. loss: 0.11846254244446755\n",
      "1700/2000. loss: 0.0572957668453455\n",
      "1710/2000. loss: 0.09889876544475555\n",
      "1720/2000. loss: 0.06367493979632854\n",
      "1730/2000. loss: 0.05323762446641922\n",
      "1740/2000. loss: 0.07504568248987198\n",
      "1750/2000. loss: 0.06468274667859078\n",
      "1760/2000. loss: 0.059599217399954794\n",
      "1770/2000. loss: 0.059918757528066635\n",
      "1780/2000. loss: 0.05760787017643452\n",
      "1790/2000. loss: 0.07437140382826328\n",
      "1800/2000. loss: 0.08051284812390805\n",
      "1810/2000. loss: 0.06216066740453243\n",
      "1820/2000. loss: 0.054119785130023954\n",
      "1830/2000. loss: 0.03958769794553518\n",
      "1840/2000. loss: 0.0610985092818737\n",
      "1850/2000. loss: 0.07774451188743114\n",
      "1860/2000. loss: 0.05293553471565247\n",
      "1870/2000. loss: 0.047929166443645956\n",
      "1880/2000. loss: 0.08100075908005237\n",
      "1890/2000. loss: 0.047937449812889096\n",
      "1900/2000. loss: 0.05256446097046137\n",
      "1910/2000. loss: 0.05138719752430916\n",
      "1920/2000. loss: 0.06314049437642097\n",
      "1930/2000. loss: 0.05075293797999621\n",
      "1940/2000. loss: 0.0492177652195096\n",
      "1950/2000. loss: 0.047789253666996954\n",
      "1960/2000. loss: 0.041509103029966354\n",
      "1970/2000. loss: 0.07577241286635399\n",
      "1980/2000. loss: 0.056394287571311\n",
      "1990/2000. loss: 0.04188943207263947\n",
      "2000/2000. loss: 0.04552941396832466\n",
      "Meta_testing finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.100764856, 0.608354344367981, 0.90853333)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case\n",
    "K = 20\n",
    "score_dict  = {'SCORE':multivariate_Normal_score,'mean':torch.zeros(1,1), 'cov':torch.ones(1,1)}\n",
    "tasks = ODE_Task_Distribution(a_min=0, a_max=1)\n",
    "# MCV\n",
    "inlr = 0.01\n",
    "instep=1\n",
    "mCV20 = MetaNeuralCV_ode('ode', MetaNeuralCVModel_ode, D_in=1, h_dims=[80] * 3, init_val=torch.tensor([0.]), weight_decay=5.e-6,\\\n",
    "                   tasks=tasks, inner_optim=torch.optim.Adam, inner_lr=inlr, meta_lr=2e-3, K=K, inner_steps=instep,\\\n",
    "                   tasks_per_meta_batch=5,\\\n",
    "                   **score_dict)\n",
    "mCV20.main_loop(num_iterations=2000)\n",
    "mCV20.test(100, inner_optim=torch.optim.Adam, inner_steps=instep)\n",
    "np.mean(np.array(mCV20.log_test[0]['Abserr_CVests'])), np.mean(np.array(mCV20.log_test[0]['Abserr_MC_2m_ests'])), np.mean(np.array(mCV20.log_test[0]['Abserr_MCests']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100\n",
      "11/100\n",
      "21/100\n",
      "31/100\n",
      "41/100\n",
      "51/100\n",
      "61/100\n",
      "71/100\n",
      "81/100\n",
      "91/100\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([0.6043, 0.8585])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NCV\n",
    "K = 20\n",
    "score_dict  = {'SCORE':multivariate_Normal_score,'mean':torch.zeros(1,1), 'cov':torch.ones(1,1)}\n",
    "tasks = ODE_Task_Distribution(a_min=0, a_max=1)\n",
    "out20 = torch.zeros(100,2)\n",
    "for i in range(100):\n",
    "    if i%10 ==0:\n",
    "        print('{}/{}'.format(i+1,100))\n",
    "    nCV = NeuralCV_ode('ode', NeuralCVModel_ode, D_in=1, h_dims=[80] * 3, init_val=torch.tensor([0.]), weight_decay=5.e-6, \\\n",
    "                   tasks=tasks, optim=torch.optim.Adam, lr=2e-3, K=K,\\\n",
    "                   **score_dict)\n",
    "    __ = nCV.train_val(num_epochs=20, batch_size=5, verbose=False)\n",
    "    out20[i,0] = torch.tensor(__[0])\n",
    "    out20[i,1] = torch.tensor(__[1])\n",
    "\n",
    "# Mean Absolute error: MC_2m abs err | NCV_abs_err\n",
    "out20.mean(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "no_tasks = 100\n",
    "set_of_ss = [2,5, 10, 20]\n",
    "no_ss = len(set_of_ss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/52whq67n6vb7v7yv1nxd7zr40000gq/T/ipykernel_98917/3879380279.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MCV_data_df = MCV_data_df.append(cur_MCV_data_df)\n",
      "/var/folders/82/52whq67n6vb7v7yv1nxd7zr40000gq/T/ipykernel_98917/3879380279.py:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  NCV_data_df= NCV_data_df.append(cur_NCV_data_df)\n",
      "/var/folders/82/52whq67n6vb7v7yv1nxd7zr40000gq/T/ipykernel_98917/3879380279.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MC_data_df= MC_data_df.append(cur_MC_data_df)\n",
      "/var/folders/82/52whq67n6vb7v7yv1nxd7zr40000gq/T/ipykernel_98917/3879380279.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MCV_data_df = MCV_data_df.append(cur_MCV_data_df)\n",
      "/var/folders/82/52whq67n6vb7v7yv1nxd7zr40000gq/T/ipykernel_98917/3879380279.py:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  NCV_data_df= NCV_data_df.append(cur_NCV_data_df)\n",
      "/var/folders/82/52whq67n6vb7v7yv1nxd7zr40000gq/T/ipykernel_98917/3879380279.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MC_data_df= MC_data_df.append(cur_MC_data_df)\n",
      "/var/folders/82/52whq67n6vb7v7yv1nxd7zr40000gq/T/ipykernel_98917/3879380279.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MCV_data_df = MCV_data_df.append(cur_MCV_data_df)\n",
      "/var/folders/82/52whq67n6vb7v7yv1nxd7zr40000gq/T/ipykernel_98917/3879380279.py:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  NCV_data_df= NCV_data_df.append(cur_NCV_data_df)\n",
      "/var/folders/82/52whq67n6vb7v7yv1nxd7zr40000gq/T/ipykernel_98917/3879380279.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MC_data_df= MC_data_df.append(cur_MC_data_df)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(no_ss):\n",
    "    if set_of_ss[i] == 2:\n",
    "        mCV_log = mCV2\n",
    "        nCV_log = out2\n",
    "    elif set_of_ss[i] == 5:\n",
    "        mCV_log = mCV5\n",
    "        nCV_log = out5\n",
    "    elif set_of_ss[i] == 10:\n",
    "        mCV_log = mCV10\n",
    "        nCV_log = out10\n",
    "    elif set_of_ss[i] == 20:\n",
    "        mCV_log = mCV20\n",
    "        nCV_log = out20\n",
    "    cur_MCV_data = list(zip(np.array(mCV_log.log_test[0]['Abserr_CVests']), np.repeat('MCV', no_tasks), np.repeat(\"N={}\".format(set_of_ss[i]*2), no_tasks)))\n",
    "    cur_MCV_data_df = pd.DataFrame(data=cur_MCV_data, columns=['est_abserr', 'method', 'sample_size'])\n",
    "    if i == 0:\n",
    "        MCV_data_df = cur_MCV_data_df\n",
    "    if i >= 1:\n",
    "        MCV_data_df = MCV_data_df.append(cur_MCV_data_df)\n",
    "\n",
    "\n",
    "    # NCV\n",
    "    cur_NCV_data = list(zip(nCV_log[:,1].numpy(), np.repeat('NCV', no_tasks), np.repeat(\"N={}\".format(set_of_ss[i]*2), no_tasks)))\n",
    "    cur_NCV_data_df = pd.DataFrame(data= cur_NCV_data, columns=['est_abserr', 'method', 'sample_size'])\n",
    "    if i == 0:\n",
    "        NCV_data_df = cur_NCV_data_df\n",
    "    if i >= 1:\n",
    "        NCV_data_df= NCV_data_df.append(cur_NCV_data_df)\n",
    "\n",
    "    # MC\n",
    "    cur_MC_data =  list(zip(nCV_log[:,0].numpy(), np.repeat('MC', no_tasks), np.repeat(\"N={}\".format(set_of_ss[i]*2), no_tasks)))\n",
    "    cur_MC_data_df = pd.DataFrame(data=cur_MC_data,columns=['est_abserr', 'method', 'sample_size'])\n",
    "    if i == 0:\n",
    "        MC_data_df = cur_MC_data_df\n",
    "    if i >= 1:\n",
    "        MC_data_df= MC_data_df.append(cur_MC_data_df)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/52whq67n6vb7v7yv1nxd7zr40000gq/T/ipykernel_98917/838035116.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  my_ODE_DF = MCV_data_df.append([MC_data_df, NCV_data_df])\n"
     ]
    }
   ],
   "source": [
    "# Merge into one giant dataset\n",
    "my_ODE_DF = MCV_data_df.append([MC_data_df, NCV_data_df])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "my_ODE_DF\n",
    "# Save data\n",
    "my_ODE_DF.to_pickle(\"ODE_DF2.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}